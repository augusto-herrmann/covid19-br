[ðŸ‡ºðŸ‡¸ English?](README.en.md)

# covid19-br

![pytest@docker](https://github.com/turicas/covid19-br/workflows/pytest@docker/badge.svg) ![goodtables](https://github.com/turicas/covid19-br/workflows/goodtables/badge.svg)

Esse repositÃ³rio centraliza links e dados sobre boletins de nÃºmero de casos das
Secretarias Estaduais de SaÃºde (SES) sobre os casos de covid19 no Brasil (por
municÃ­pio por dia), alÃ©m de outros dados relevantes para a anÃ¡lise, como Ã³bitos
registrados em cartÃ³rio (por estado por dia).

## LicenÃ§a e CitaÃ§Ã£o

A licenÃ§a do cÃ³digo Ã© [LGPL3](https://www.gnu.org/licenses/lgpl-3.0.en.html) e
dos dados convertidos [Creative Commons Attribution
ShareAlike](https://creativecommons.org/licenses/by-sa/4.0/). Caso utilize os
dados, **cite a fonte original e quem tratou os dados** e caso compartilhe os
dados, **utilize a mesma licenÃ§a**.
Exemplos de como os dados podem ser citados:
- **Fonte: Secretarias de SaÃºde das Unidades Federativas, dados tratados por Ãlvaro Justen e equipe de voluntÃ¡rios [Brasil.IO](https://brasil.io/)**
- **Brasil.IO: boletins epidemiolÃ³gicos da COVID-19 por municÃ­pio por dia, disponÃ­vel em: https://brasil.io/dataset/covid19/ (Ãºltima atualizaÃ§Ã£o: XX de XX de XXXX, acesso em XX de XX de XXXX).**


## Dados

Depois de coletados e checados os dados ficam disponÃ­veis de 3 formas no
[Brasil.IO](https://brasil.io/):

- [Interface Web](https://brasil.io/dataset/covid19) (feita para humanos)
- [API](https://brasil.io/api/dataset/covid19) (feita para humanos que desenvolvem programas) - [veja a documentaÃ§Ã£o da API](api.md)
- [Download do dataset completo](https://data.brasil.io/dataset/covid19/_meta/list.html)

Caso queira acessar os dados antes de serem publicados (ATENÃ‡ÃƒO: pode ser que
nÃ£o tenham sido checados), vocÃª pode [acessar diretamente as planilhas em que
estamos
trabalhando](https://drive.google.com/open?id=1l3tiwrGEcJEV3gxX0yP-VMRNaE1MLfS2).

Se esse programa e/ou os dados resultantes foram Ãºteis a vocÃª ou Ã  sua empresa,
**considere [fazer uma doaÃ§Ã£o ao projeto Brasil.IO](https://brasil.io/doe)**,
que Ã© mantido voluntariamente.


### FAQ SOBRE OS DADOS

**Antes de entrar em contato conosco (estamos sobrecarregados) para tirar
dÃºvidas sobre os dados, [CONSULTE NOSSO FAQ](faq.md).**

Para mais detalhes [veja a metodologia de coleta de
dados](https://drive.google.com/open?id=1escumcbjS8inzAKvuXOQocMcQ8ZCqbyHU5X5hFrPpn4).

### Analisando os dados

Caso queira analisar os dados usando SQL, veja o script
[`analysis.sh`](analysis.sh) (ele baixa e converte os CSVs para um banco de
dados SQLite e jÃ¡ cria Ã­ndices e *views* que facilitam o trabalho) e os
arquivos na pasta [`sql/`](sql/). Por padrÃ£o o script reutiliza os arquivos
caso jÃ¡ tenha baixado; para sempre baixar a versÃ£o mais atual dos dados,
execute `./analysis.sh --clean`.

### Validando os dados

Os metadados estÃ£o descritos conforme os padrÃµes *Data Package* e
*[Table Schema](https://specs.frictionlessdata.io/table-schema/#language)* do
*[Frictionless Data](https://frictionlessdata.io/)*. Isso significa que os
dados podem ser validados automaticamente para detectar, por exemplo, se os
valores de um campo estÃ£o em conformidade com a tipagem definida, se uma data
Ã© vÃ¡lida, se hÃ¡ colunas faltando ou se hÃ¡ linhas duplicadas.

Para fazer a verificaÃ§Ã£o, ative o ambiente virtual Python e em seguida digite:

```
goodtables data/datapackage.json
```

O relatÃ³rio da ferramenta
*[Good Tables](https://github.com/frictionlessdata/goodtables-py)* irÃ¡ indicar
se houver alguma inconsistÃªncia. A validaÃ§Ã£o tambÃ©m pode ser feita *online*
pelo site [Goodtables.io](http://goodtables.io/).

## Contribuindo

VocÃª pode contribuir de diversas formas:

- Criando programas (crawlers/scrapers/spiders) para extrair os dados automaticamente ([LEIA ISSO ANTES](#criando-scrapers));
- Coletando links para os boletins de seu estado;
- Coletando dados sobre os casos por municÃ­pio por dia;
- Entrando em contato com a secretaria estadual de seu estado, sugerindo as
  [recomendaÃ§Ãµes de liberaÃ§Ã£o dos dados](recomendacoes.md);
- Evitando contato com humanos;
- Lavando as mÃ£os vÃ¡rias vezes ao dia;
- Sendo solidÃ¡rio aos mais vulnerÃ¡veis;

Para se voluntariar, [siga estes passos](CONTRIBUTING.md).

Procure o seu estado [nas issues desse
repositÃ³rio](https://github.com/turicas/covid19-br/issues) e vamos conversar
por lÃ¡.

### Criando Scrapers

Estamos mudando a forma de subida dos dados para facilitar o trabalho dos voluntÃ¡rios e deixar o processo mais robusto e confiÃ¡vel e, com isso, serÃ¡ mais fÃ¡cil que robÃ´s possam subir tambÃ©m os dados; dessa forma, os scrapers ajudarÃ£o *bastante* no processo. PorÃ©m, ao criar um scraper Ã© importante que vocÃª siga algumas regras:

- **NecessÃ¡rio** fazer o scraper usando o `scrapy`;
- **NÃ£o usar** `pandas`, `BeautifulSoap`, `requests` ou outras bibliotecas
  desnecessÃ¡rias (a std lib do Python jÃ¡ tem muita biblioteca Ãºtil, o `scrapy`
  com XPath jÃ¡ dÃ¡ conta de boa parte das raspagens e `rows` jÃ¡ Ã© uma
  dependÃªncia desse repositÃ³rio);
- Criar um arquivo `web/spiders/spider_xx.py`, onde `xx` Ã© a sigla do estado,
  em minÃºsculas. Criar uma nova classe e herdar da classe `BaseCovid19Spider`,
  do `base.py`. A sigla do estado, com 2 caracteres maiÃºsculos, deve ser um
  atributo da classe do spider e usar `self.state`. Veja os exemplos jÃ¡
  implementados;
- Deve existir alguma maneira fÃ¡cil de fazer o scraper coletar os boletins e
  casos para uma data especÃ­fica (mas ele deve ser capaz de identificar para
  quais datas os dados disponÃ­veis e de capturar vÃ¡rias datas tambÃ©m);
- A leitura pode ser feita a partir de contagens por municÃ­pio ou de microdados
  de casos individuais. Neste caso, Ã© necessÃ¡rio que o prÃ³prio scraper calcule
  os totais por municÃ­pio;
- O mÃ©todo `parse` deve chamar o mÃ©todo `self.add_report(date, url)`, sendo
  `date` a data do relatÃ³rio e `url` a URL da fonte de informaÃ§Ã£o;
- Para cada municÃ­pio no estado, chamar o mÃ©todo `self.add_city_case` com os
  seguintes parÃ¢metros:
  - `city`: nome do municÃ­pio
  - `confirmed`: inteiro, nÃºmero de casos confirmados (ou `None`)
  - `death`: inteiro, nÃºmero de Ã³bitos naquele dia (ou `None`)
- Ler os totais do estado a partir da fonte da informaÃ§Ã£o, se estiver
  disponÃ­vel. Deve-se somar os nÃºmeros de cada municÃ­pio *somente se essa
  informaÃ§Ã£o nÃ£o estiver disponÃ­vel na fonte original*. Incluir os nÃºmeros
  totais no estado chamando o mÃ©todo
  `self.add_state_case`. Os parÃ¢metros sÃ£o os mesmos do mÃ©todo usado para o
  municÃ­pio, exceto pela omissÃ£o do parÃ¢metro `city`;
- Quando possÃ­vel, use testes automatizados.

Nesse momento nÃ£o temos muito tempo disponÃ­vel para revisÃ£o, entÃ£o **por favor**, sÃ³ crie um *pull request* com cÃ³digo de um novo scraper caso vocÃª possa cumprir os requisitos acima.

## Instalando

### PadrÃ£o

Necessita de Python 3 (testado em 3.8.2). Para montar seu ambiente:

- Instale o Python 3.8.2
- Crie um virtualenv
- Instale as dependÃªncias: `pip install -r requirements.txt`
- Rode o script de coleta: `./run-spiders.sh`
- Rode o script de consolidaÃ§Ã£o: `./run.sh`

Verifique o resultado em `data/output`.

### Docker

Se vocÃª preferir utilizar o Docker para executar, basta usar os comandos a seguir :

```shell
make docker-build       # para construir a imagem
make docker-run-spiders # para coletar os dados
make docker-run         # para consolidar os dados
```

## VEJA TAMBÃ‰M

- [Outros datasets relevantes](datasets-relevantes.md)
- [RecomendaÃ§Ãµes para secretarias de saÃºde na disponibilizaÃ§Ã£o de
  dados](recomendacoes.md)


## Clipping

Quer saber quais projetos e notÃ­cias estÃ£o usando nossos dados? [Veja o
clipping](clipping.md).


## AtualizaÃ§Ã£o dos Dados no Brasil.IO

Crie um arquivo `.env` com os valores corretos para as seguintes variÃ¡veis de
ambiente:

```shell
BRASILIO_SSH_USER
BRASILIO_SSH_SERVER
BRASILIO_DATA_PATH
BRASILIO_UPDATE_COMMAND
```

Execute o script:

`./deploy.sh full`

Ele irÃ¡ coletar os dados das planilhas (que estÃ£o linkadas em
`data/boletim_url.csv` e `data/caso_url.csv`), adicionar os dados ao
repositÃ³rio, compactÃ¡-los, enviÃ¡-los ao servidor e executar o comando de
atualizaÃ§Ã£o de dataset.

> Nota: o script que baixa e converte os dados automaticamente deve ser
> executado separadamente, com o comando `./run-spiders.sh`.
